import numpy as np
from keras.layers import Flatten, Conv2D, Input, Dense
from keras.models import Model
from keras.optimizers import Adam
from agents.base import Agent
import tensorflow as tf
from keras import backend as K
from tqdm import tqdm


def kullback_leibler_divergence(y_true, y_pred):
    tau = 0.1

    y_true = K.softmax(y_true / tau)
    y_pred = K.softmax(y_pred)

    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)


class SingleDtStudent(Agent):
    """ student that distill from single teacher """

    def __init__(self, env, lr, logger, batch_size, epsilon, teacher, add_mem_num, update_num, epoch, loss_fuc):
        """ initiated by a teacher  """
        super().__init__(env, logger)

        self.batch_size = batch_size

        self.eps = epsilon

        self.teacher = teacher

        # the amount of memory we add for each epoch
        self.add_mem_num = add_mem_num
        # the number of times of update for each epock
        self.update_num = update_num

        self.epoch = epoch

        self.loss_fuc_name = loss_fuc

        assert loss_fuc in ['mse', 'kld'], "Not supported loss function"

        if loss_fuc == 'kld':
            loss_fuc = kullback_leibler_divergence

        # build model
        self.model = self.build_CNN_model(self.state_shape, self.action_num, "student")
        self.model.compile(optimizer=Adam(lr), loss=loss_fuc)

        # set logger
        if logger is not None:
            logger.set_model(self.model)
            logger.set_loss_name([*self.model.metrics_names])

    def distill(self):
        """ Supervised learning by the memory generated by teacher """

        for e in tqdm(range(self.epoch)):

            self.teacher.add_memories(size=self.add_mem_num)

            total_loss = 0
            for i in range(self.update_num):
                s_batch, o_batch = self.teacher.sample_memories()

                s_batch = self._LazyFrame2array(s_batch)
                o_batch = np.array(o_batch)

                loss = self.model.train_on_batch(s_batch, o_batch)

                total_loss += loss

                if self.logger is not None:
                    self.logger.add_loss([loss])

            print("*** Epoch:{} ,Memory-size: {},avg_loss: {} ***".format(e + 1, self.teacher.get_memory_size(),
                                                                          total_loss / self.update_num))

    def select_action(self, state):
        state = self._LazyFrame2array(state)
        output = self.model.predict_on_batch(np.expand_dims(state, axis=0))

        return np.argmax(output[0])

    def _LazyFrame2array(self, LazyFrame):
        return np.array(LazyFrame)


if __name__ == '__main__':
    config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True, visible_device_list='0'))
    sess = tf.Session(config=config)
    K.set_session(sess)

    a = np.array([[1, 2, 3, 4, 5]]).astype(np.float32)
    b = K.softmax(a)
    c = sess.run(b)

    print(c)
