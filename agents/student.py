import numpy as np
from keras.layers import Flatten, Conv2D, Input, Dense
from keras.models import Model
from keras.optimizers import Adam
from agents.base import Agent,ModelBuilder
import tensorflow as tf
from keras import backend as K
from tqdm import tqdm
import os


def kullback_leibler_divergence(y_true, y_pred):
    tau = 0.1

    y_true = K.softmax(y_true / tau)
    y_pred = K.softmax(y_pred)

    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)


class SingleDtStudent(Agent):
    """ student that distill from single teacher """

    def __init__(self, env, lr, logger, batch_size, epsilon, teacher, add_mem_num, update_num, epoch, loss_fuc,target_net_size):
        """ initiated by a teacher  """
        super().__init__(env, logger)

        self.batch_size = batch_size

        self.eps = epsilon

        self.teacher = teacher

        # the amount of memory we add for each epoch
        self.add_mem_num = add_mem_num
        # the number of times of update for each epock
        self.update_num = update_num

        self.epoch = epoch

        assert loss_fuc in ['mse', 'kld'], "Not supported loss function"

        if loss_fuc == 'kld':
            loss_fuc = kullback_leibler_divergence

        # build model
        assert target_net_size in ['big','small','normal','super'],"net_size must be one of ['big','small','normal']"

        if target_net_size == 'big':
            self.model = self.build_big_CNN_model(self.state_shape, self.action_num, "model")
        elif target_net_size == 'small':
            self.model = self.build_small_CNN_model(self.state_shape, self.action_num, "model")
        elif target_net_size == 'normal':
            self.model = self.build_CNN_model(self.state_shape, self.action_num, "model")
        elif target_net_size == 'super':
            self.model = self.build_super_CNN_model(self.state_shape, self.action_num, "model")
        self.model.compile(Adam(lr),loss = kullback_leibler_divergence)
        # set logger
        if logger is not None:
            logger.set_model(self.model)
            logger.set_loss_name([*self.model.metrics_names])

    def distill(self):
        """ Supervised learning by the memory generated by teacher """


        print('adding memory')
        self.teacher.add_memories(size=self.add_mem_num)

        total_loss = 0
        for i in tqdm(range(self.update_num)):
            s_batch, o_batch = self.teacher.sample_memories()

            s_batch = self._LazyFrame2array(s_batch)
            o_batch = np.array(o_batch)

            loss = self.model.train_on_batch(s_batch, o_batch)

            total_loss += loss

            if self.logger is not None:
                self.logger.add_loss([loss])

    def select_action(self, state):
        state = self._LazyFrame2array(state)
        output = self.model.predict_on_batch(np.expand_dims(state, axis=0)).ravel()
        return np.argmax(output)

    def _LazyFrame2array(self, LazyFrame):
        return np.array(LazyFrame)

class SingleDtStudent_world(ModelBuilder):

    def __init__(self,teacher,logger,net_size,epoch,lr):

        self.teacher = teacher

        self.logger=logger

        self.state_shape=(84,84,4)

        self.action_num = 4

        self.epoch = epoch

        if net_size == 'big':
            self.model = self.build_big_CNN_model(self.state_shape, self.action_num, "model")
        elif net_size == 'small':
            self.model = self.build_small_CNN_model(self.state_shape, self.action_num, "model")
        elif net_size == 'normal':
            self.model = self.build_CNN_model(self.state_shape, self.action_num, "model")
        
        self.model.compile(optimizer=Adam(lr), loss=kullback_leibler_divergence)

        logger.set_model(self.model)
        logger.set_loss_name([*self.model.metrics_names])

    def distill(self):
        
        avg_loss = 0
        for e in tqdm(range(self.epoch),ascii=True):

            s_batch, o_batch = self.teacher.sample_memories()

            loss = self.model.train_on_batch(np.array(s_batch),np.array(o_batch))

            self.logger.add_loss([loss])

            avg_loss+=loss

            if e%1000 == 0:
                print(" avg_loss: {}".format(avg_loss/1000))
                avg_loss=0
            
            





if __name__ == '__main__':
    config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True, visible_device_list='0'))
    sess = tf.Session(config=config)
    K.set_session(sess)

    a = np.array([[1, 2, 3, 4, 5]]).astype(np.float32)
    b = K.softmax(a)
    c = sess.run(b)

    print(c)
